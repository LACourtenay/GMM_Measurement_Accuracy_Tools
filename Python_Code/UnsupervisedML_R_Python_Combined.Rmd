---
title: "Clustering Algorithms"
author: "Lloyd Austin Courtenay"
date: "11/3/2020"
output: html_document
---

```{r}

library(geomorph)
library(shapes)
library(tibble)
library(dplyr)
library(tidyverse)
library(ggplot2)

directory<-"C:\\"

a<-read.morphologika(paste(directory, "All_Landmarks.txt", sep = "\\"))
GPA<-gpagen(a$coords, curves = NULL, surfaces = NULL, PrinAxes = TRUE,
            max.iter = NULL, ProcD = TRUE, Proj = TRUE, print.progress = TRUE)
data<-tibble(label = character(), Landmark = factor(), Sample = factor(), Animal = factor(),
             x = numeric(), y = numeric(), z = numeric())
for (i in 1:length(a$labels)){
  sep<-GPA$coords[,,i]
  for (i2 in 1:nrow(sep)){
    LM<-sep[i2,]
    data<-add_row(data, label = rownames(a$labels)[i],
                  Landmark = paste("LM", i2, sep = ""),
                  Sample = a$labels[i],
                  Animal = if_else(grepl("Wolf", label) == TRUE, "Wolf", "Dog"),
                  x = LM[1], y = LM[2], z = LM[3])
  }
}; data<-as.tibble(data) %>%
  mutate(label = if_else(grepl("Wolf", label) == TRUE,
                               substr(label, 1, 11), substr(label, 1, 12))); data$label<-factor(data$label); rm(sep, LM)

# Write fully superimposed procrustes coordinates to disk

write.table(data, paste(directory, "proc_coords.txt", sep = "\t"))

```

```{r}
library(reticulate)
use_virtualenv("tensorflow") # If a conda virtual environment is available
py_available(TRUE)
# Ensure that knitr and rmarkdown packages are updated and fully installed.
```

```{python}
import os
import pandas as pd
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import sklearn
from sklearn.cluster import DBSCAN, KMeans, MeanShift, AgglomerativeClustering, estimate_bandwidth
import scipy
from scipy.spatial.distance import cdist
from scipy import stats
print("Imported Packages:", "\nNumpy: {}.".format(np.__version__),
      "\nMatplotlib: {}.".format(matplotlib.__version__),
      "\nSciPy: {}.".format(scipy.__version__),
      "\nScikit-learn: {}.".format(sklearn.__version__))

# set matplotlib parameters

plt.style.use("ggplot")
plt.rcParams["figure.figsize"] = (16,12)
plt.rcParams["axes.facecolor"] = "None"
```

```{python}
os.chdir("C:/")
data = pd.read_csv("proc_coords.txt", sep = "\t")
all_data = data[["Landmark","x","y","z"]]

outline_data = all_data[all_data["Landmark"] != "LM14"]
outline_data = outline_data[outline_data["Landmark"] != "LM15"]
outline_data = outline_data[outline_data["Landmark"] != "LM16"]
outline_data = outline_data[outline_data["Landmark"] != "LM17"]

core_data = outline_data[outline_data["Landmark"] != "LM13"]
core_data = core_data[core_data["Landmark"] != "LM12"]
core_data = core_data[core_data["Landmark"] != "LM11"]
core_data = core_data[core_data["Landmark"] != "LM10"]
core_data = core_data[core_data["Landmark"] != "LM9"]
core_data = core_data[core_data["Landmark"] != "LM8"]
core_data = core_data[core_data["Landmark"] != "LM7"]
core_data = core_data[core_data["Landmark"] != "LM6"]

```

```{r}
par(mfrow = c(1,3))
plot(py$all_data[["x"]], py$all_data[["y"]])
plot(py$outline_data[["x"]], py$outline_data[["y"]])
plot(py$core_data[["x"]], py$core_data[["y"]])
```

```{python}

raw_data = pd.DataFrame.to_numpy(all_data[["x","y","z"]], dtype = "float64")

db = DBSCAN(eps = 0.04, min_samples = 30).fit(raw_data)

core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)
n_noise_ = list(labels).count(-1)
unique_labels = set(labels)
colors = [plt.cm.Spectral(each)
          for each in np.linspace(0, 1, len(unique_labels))]
for k, col in zip(unique_labels, colors):
  if k == -1:
  # Black used for noise.
    col = [0, 0, 0, 1]
  class_member_mask = (labels == k)
  xy = raw_data[class_member_mask & core_samples_mask]
  plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),
           markeredgecolor='k', markersize=5)
  xy = raw_data[class_member_mask & ~core_samples_mask]
  plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),
           markeredgecolor='k', markersize=1)
plt.title('Estimated number of clusters ({}) and noise points ({})'.format(n_clusters_, n_noise_))
plt.axis("off")
plt.show()
stats.find_repeats(labels)

```

```{r}

library(tidyverse)
library(dplyr)
library(tibble)

all_data_DBSCAN<-tibble(LM = py$all_data[["Landmark"]],
                        Cluster = py$labels)
table(all_data_DBSCAN$LM, all_data_DBSCAN$Cluster)

```

```{python}

raw_data = pd.DataFrame.to_numpy(outline_data[["x","y","z"]], dtype = "float64")

db = DBSCAN(eps = 0.04, min_samples = 30).fit(raw_data)

core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)
n_noise_ = list(labels).count(-1)
unique_labels = set(labels)
colors = [plt.cm.Spectral(each)
          for each in np.linspace(0, 1, len(unique_labels))]
for k, col in zip(unique_labels, colors):
  if k == -1:
  # Black used for noise.
    col = [0, 0, 0, 1]
  class_member_mask = (labels == k)
  xy = raw_data[class_member_mask & core_samples_mask]
  plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),
           markeredgecolor='k', markersize=5)
  xy = raw_data[class_member_mask & ~core_samples_mask]
  plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),
           markeredgecolor='k', markersize=1)
plt.title('Estimated number of clusters ({}) and noise points ({})'.format(n_clusters_, n_noise_))
plt.axis("off")
plt.show()
stats.find_repeats(labels)
```

```{r}
all_data_DBSCAN<-tibble(LM = py$outline_data[["Landmark"]],
                        Cluster = py$labels)
table(all_data_DBSCAN$LM, all_data_DBSCAN$Cluster)

```

```{python}

raw_data = pd.DataFrame.to_numpy(core_data[["x","y","z"]], dtype = "float64")

db = DBSCAN(eps = 0.04, min_samples = 30).fit(raw_data)

core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)
n_noise_ = list(labels).count(-1)
unique_labels = set(labels)
colors = [plt.cm.Spectral(each)
          for each in np.linspace(0, 1, len(unique_labels))]
for k, col in zip(unique_labels, colors):
  if k == -1:
  # Black used for noise.
    col = [0, 0, 0, 1]
  class_member_mask = (labels == k)
  xy = raw_data[class_member_mask & core_samples_mask]
  plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),
           markeredgecolor='k', markersize=5)
  xy = raw_data[class_member_mask & ~core_samples_mask]
  plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),
           markeredgecolor='k', markersize=1)
plt.title('Estimated number of clusters ({}) and noise points ({})'.format(n_clusters_, n_noise_))
plt.axis("off")
plt.show()
stats.find_repeats(labels)
```

```{r}
all_data_DBSCAN<-tibble(LM = py$core_data[["Landmark"]],
                        Cluster = py$labels)
table(all_data_DBSCAN$LM, all_data_DBSCAN$Cluster)

```

```{python}

raw_data = pd.DataFrame.to_numpy(all_data[["x","y","z"]], dtype = "float64")

ms = MeanShift(cluster_all = False, bandwidth = estimate_bandwidth(raw_data, quantile = .05, n_samples = raw_data.shape[0]))
y_pred = ms.fit_predict(raw_data)

labels = ms.labels_
n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)
n_noise_ = list(labels).count(-1)
plt.scatter(raw_data[:, 0], raw_data[:, 1], c = y_pred)
plt.title('Estimated number of clusters ({}) and noise points ({})'.format(n_clusters_, n_noise_))
plt.axis("off")
plt.show()
```
```{r}
all_data_DBSCAN<-tibble(LM = py$all_data[["Landmark"]],
                        Cluster = py$labels)
table(all_data_DBSCAN$LM, all_data_DBSCAN$Cluster)

```

```{python}

raw_data = pd.DataFrame.to_numpy(outline_data[["x","y","z"]], dtype = "float64")

ms = MeanShift(cluster_all = False, bandwidth = estimate_bandwidth(raw_data, quantile = .05, n_samples = raw_data.shape[0]))
y_pred = ms.fit_predict(raw_data)

labels = ms.labels_
n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)
n_noise_ = list(labels).count(-1)
plt.scatter(raw_data[:, 0], raw_data[:, 1], c = y_pred)
plt.title('Estimated number of clusters ({}) and noise points ({})'.format(n_clusters_, n_noise_))
plt.axis("off")
plt.show()
```

```{r}
all_data_DBSCAN<-tibble(LM = py$outline_data[["Landmark"]],
                        Cluster = py$labels)
table(all_data_DBSCAN$LM, all_data_DBSCAN$Cluster)

```

```{python}

raw_data = pd.DataFrame.to_numpy(core_data[["x","y","z"]], dtype = "float64")

ms = MeanShift(cluster_all = False, bandwidth = estimate_bandwidth(raw_data, quantile = .1, n_samples = raw_data.shape[0]))
y_pred = ms.fit_predict(raw_data)

labels = ms.labels_
n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)
n_noise_ = list(labels).count(-1)
plt.scatter(raw_data[:, 0], raw_data[:, 1], c = y_pred)
plt.title('Estimated number of clusters ({}) and noise points ({})'.format(n_clusters_, n_noise_))
plt.axis("off")
plt.show()
```

```{r}
all_data_DBSCAN<-tibble(LM = py$core_data[["Landmark"]],
                        Cluster = py$labels)
table(all_data_DBSCAN$LM, all_data_DBSCAN$Cluster)

```

```{python}

data_to_plot = np.stack((raw_data[:,0], raw_data[:,1], labels), axis = 1)

```

```{r}
data_for_plot<-py$data_to_plot
data_for_plot$label<-as.factor(DBSCAN_all$label)

points<-data_for_plot %>% filter(label != "-1"); hull<-points %>% group_by(label) %>% slice(chull(x,y))
noise<-data_for_plot %>% filter(label == "-1")
centroids<-tibble(x = numeric(), y = numeric())
for (i in 1:length(levels(points$label))) {
  split_label<-split(points, points$label)
  split_label<-split_label[[i]]
  centroids<-add_row(centroids, x = mean(as.matrix(split_label[1])),
                     y = mean(as.matrix(split_label[2])))
  }; centroids<-centroids[2:nrow(centroids),] %>% as.data.frame
ggplot(data = points, aes(x = x, y = y, colour = label)) +
  geom_point(stat = "identity", size = 1.5, shape = 19) +
  geom_polygon(data = hull, aes(x = x, y = y), alpha = 0.2, size = 1) +
  geom_point(data = noise, aes(x = x, y = y), colour = "black",
             stat = "identity", size = 1, shape = 19) +
  geom_point(data = centroids, aes(x = x, y = y), colour = "black", stat = "identity",
             size = 15, shape = "*") +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.background = element_blank(),
        plot.margin = unit(c(1,1,1,1), "cm"),
        plot.title = element_text(face = "bold", size = 20),
        plot.subtitle = element_text(size = 15),
        panel.border = element_rect(colour = "black", fill = NA),
        axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.x = element_blank(),
        axis.ticks.y = element_blank(),
        legend.position = "none") +
  coord_flip() +
  scale_y_reverse()

```
